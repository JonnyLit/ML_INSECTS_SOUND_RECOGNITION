/home/zord/anaconda3/bin/python /home/zord/PycharmProjects/SBCM_4_classes/main.py 
2025-09-20 09:39:37.671998: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
features_queen_absent.shape: (54637, 32, 44)
queen_queen_absent.shape: (54637,)
features_queen_present_newly_accepted.shape: (54560, 32, 44)
queen_queen_present_newly_accepted.shape: (54560,)
features_queen_present_original.shape: (54516, 32, 44)
queen_queen_present_original.shape: (54516,)
features_queen_present_rejected.shape: (54527, 32, 44)
queen_queen_present_rejected.shape: (54527,)
Training...
2025-09-20 09:49:54.224369: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
X_train.shape
152766 32
X_train.shape, Y_train.shape, X_test.shape, Y_test.shape
(152766, 32, 44, 1) (152766, 1) (65474, 32, 44, 1) (65474, 1)
le: LabelEncoder()
after LabelEncoder()
X_train.shape, Y_train.shape, X_test.shape, Y_test.shape
(152766, 32, 44, 1) (152766, 1) (65474, 32, 44, 1) (65474, 1)
after Y_train = to_categorical(le.fit_transform(Y_train))
X_train.shape, Y_train.shape, X_test.shape, Y_test.shape
(152766, 32, 44, 1) (152766, 4) (65474, 32, 44, 1) (65474, 1)
before model.evaluate and after Y_test = to_categorical(le.fit_transform(Y_test))
X_train.shape, Y_train.shape, X_test.shape, Y_test.shape
(152766, 32, 44, 1) (152766, 4) (65474, 32, 44, 1) (65474, 4)
/home/zord/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
/home/zord/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
2047/2047 [==============================] - 63s 30ms/step - loss: 1.3866 - accuracy: 0.2509 - precision: 0.0000e+00 - recall: 0.0000e+00
after model.evaluate
X_train.shape, Y_train.shape, X_test.shape, Y_test.shape
(152766, 32, 44, 1) (152766, 4) (65474, 32, 44, 1) (65474, 4)
Predicted accuracy:  25.08782148361206
Epoch 1/110
1194/1194 [==============================] - 499s 413ms/step - loss: 0.6392 - accuracy: 0.7438 - precision: 0.8232 - recall: 0.6559 - val_loss: 1.1151 - val_accuracy: 0.6453 - val_precision: 0.6566 - val_recall: 0.6241 - lr: 5.0000e-04
Epoch 2/110
1194/1194 [==============================] - 535s 448ms/step - loss: 0.3659 - accuracy: 0.8643 - precision: 0.8858 - recall: 0.8388 - val_loss: 3.7957 - val_accuracy: 0.4536 - val_precision: 0.4571 - val_recall: 0.4474 - lr: 5.0000e-04
Epoch 3/110
1194/1194 [==============================] - 504s 422ms/step - loss: 0.2975 - accuracy: 0.8904 - precision: 0.9049 - recall: 0.8740 - val_loss: 1.7008 - val_accuracy: 0.6282 - val_precision: 0.6399 - val_recall: 0.6122 - lr: 5.0000e-04
Epoch 4/110
1194/1194 [==============================] - 480s 402ms/step - loss: 0.2583 - accuracy: 0.9060 - precision: 0.9171 - recall: 0.8934 - val_loss: 0.4028 - val_accuracy: 0.8563 - val_precision: 0.8639 - val_recall: 0.8493 - lr: 5.0000e-04
Epoch 5/110
1194/1194 [==============================] - 471s 394ms/step - loss: 0.2340 - accuracy: 0.9155 - precision: 0.9247 - recall: 0.9057 - val_loss: 3.0312 - val_accuracy: 0.4825 - val_precision: 0.4889 - val_recall: 0.4739 - lr: 5.0000e-04
Epoch 6/110
1194/1194 [==============================] - 538s 450ms/step - loss: 0.2161 - accuracy: 0.9220 - precision: 0.9303 - recall: 0.9136 - val_loss: 0.2086 - val_accuracy: 0.9265 - val_precision: 0.9297 - val_recall: 0.9234 - lr: 5.0000e-04
Epoch 7/110
1194/1194 [==============================] - 477s 399ms/step - loss: 0.2009 - accuracy: 0.9273 - precision: 0.9344 - recall: 0.9200 - val_loss: 0.5915 - val_accuracy: 0.8188 - val_precision: 0.8260 - val_recall: 0.8116 - lr: 5.0000e-04
Epoch 8/110
1194/1194 [==============================] - 477s 400ms/step - loss: 0.1930 - accuracy: 0.9307 - precision: 0.9374 - recall: 0.9237 - val_loss: 1.8006 - val_accuracy: 0.6069 - val_precision: 0.6142 - val_recall: 0.5980 - lr: 5.0000e-04
Epoch 9/110
1194/1194 [==============================] - 469s 393ms/step - loss: 0.1808 - accuracy: 0.9355 - precision: 0.9415 - recall: 0.9294 - val_loss: 0.4521 - val_accuracy: 0.8673 - val_precision: 0.8720 - val_recall: 0.8639 - lr: 5.0000e-04
Epoch 10/110
1194/1194 [==============================] - 470s 393ms/step - loss: 0.1748 - accuracy: 0.9370 - precision: 0.9431 - recall: 0.9315 - val_loss: 0.2635 - val_accuracy: 0.9023 - val_precision: 0.9078 - val_recall: 0.8966 - lr: 5.0000e-04
Epoch 11/110
1194/1194 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9399 - precision: 0.9452 - recall: 0.9348
Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
1194/1194 [==============================] - 460s 385ms/step - loss: 0.1669 - accuracy: 0.9399 - precision: 0.9452 - recall: 0.9348 - val_loss: 2.0527 - val_accuracy: 0.5809 - val_precision: 0.5876 - val_recall: 0.5674 - lr: 5.0000e-04
Epoch 12/110
1194/1194 [==============================] - 468s 392ms/step - loss: 0.1328 - accuracy: 0.9531 - precision: 0.9568 - recall: 0.9493 - val_loss: 0.5401 - val_accuracy: 0.8535 - val_precision: 0.8583 - val_recall: 0.8497 - lr: 2.5000e-04
Epoch 13/110
1194/1194 [==============================] - 482s 403ms/step - loss: 0.1267 - accuracy: 0.9554 - precision: 0.9589 - recall: 0.9519 - val_loss: 0.1992 - val_accuracy: 0.9286 - val_precision: 0.9321 - val_recall: 0.9260 - lr: 2.5000e-04
Epoch 14/110
1194/1194 [==============================] - 524s 439ms/step - loss: 0.1239 - accuracy: 0.9557 - precision: 0.9589 - recall: 0.9525 - val_loss: 0.7385 - val_accuracy: 0.8210 - val_precision: 0.8247 - val_recall: 0.8176 - lr: 2.5000e-04
Epoch 15/110
1194/1194 [==============================] - 477s 400ms/step - loss: 0.1192 - accuracy: 0.9575 - precision: 0.9606 - recall: 0.9543 - val_loss: 0.3661 - val_accuracy: 0.8874 - val_precision: 0.8915 - val_recall: 0.8841 - lr: 2.5000e-04
Epoch 16/110
1194/1194 [==============================] - 552s 463ms/step - loss: 0.1156 - accuracy: 0.9588 - precision: 0.9618 - recall: 0.9559 - val_loss: 0.3283 - val_accuracy: 0.9080 - val_precision: 0.9114 - val_recall: 0.9061 - lr: 2.5000e-04
Epoch 17/110
1194/1194 [==============================] - 477s 399ms/step - loss: 0.1138 - accuracy: 0.9595 - precision: 0.9622 - recall: 0.9567 - val_loss: 0.8041 - val_accuracy: 0.8243 - val_precision: 0.8270 - val_recall: 0.8220 - lr: 2.5000e-04
Epoch 18/110
1194/1194 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9603 - precision: 0.9631 - recall: 0.9577
Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
1194/1194 [==============================] - 539s 452ms/step - loss: 0.1101 - accuracy: 0.9603 - precision: 0.9631 - recall: 0.9577 - val_loss: 2.0442 - val_accuracy: 0.7028 - val_precision: 0.7075 - val_recall: 0.6987 - lr: 2.5000e-04
Epoch 19/110
1194/1194 [==============================] - 536s 449ms/step - loss: 0.0917 - accuracy: 0.9672 - precision: 0.9692 - recall: 0.9654 - val_loss: 0.2490 - val_accuracy: 0.9284 - val_precision: 0.9302 - val_recall: 0.9266 - lr: 1.2500e-04
Epoch 20/110
1194/1194 [==============================] - 472s 396ms/step - loss: 0.0881 - accuracy: 0.9686 - precision: 0.9706 - recall: 0.9669 - val_loss: 0.3248 - val_accuracy: 0.9045 - val_precision: 0.9075 - val_recall: 0.9023 - lr: 1.2500e-04
Epoch 21/110
1194/1194 [==============================] - 538s 451ms/step - loss: 0.0868 - accuracy: 0.9691 - precision: 0.9710 - recall: 0.9671 - val_loss: 0.2020 - val_accuracy: 0.9407 - val_precision: 0.9429 - val_recall: 0.9392 - lr: 1.2500e-04
Epoch 22/110
1194/1194 [==============================] - 578s 484ms/step - loss: 0.0849 - accuracy: 0.9697 - precision: 0.9715 - recall: 0.9678 - val_loss: 0.5703 - val_accuracy: 0.8640 - val_precision: 0.8673 - val_recall: 0.8617 - lr: 1.2500e-04
Epoch 23/110
1194/1194 [==============================] - 469s 393ms/step - loss: 0.0847 - accuracy: 0.9697 - precision: 0.9717 - recall: 0.9678 - val_loss: 0.7243 - val_accuracy: 0.8422 - val_precision: 0.8448 - val_recall: 0.8402 - lr: 1.2500e-04
Epoch 24/110
1194/1194 [==============================] - 557s 466ms/step - loss: 0.0828 - accuracy: 0.9705 - precision: 0.9724 - recall: 0.9686 - val_loss: 0.6320 - val_accuracy: 0.8590 - val_precision: 0.8615 - val_recall: 0.8570 - lr: 1.2500e-04
Epoch 25/110
1194/1194 [==============================] - 494s 413ms/step - loss: 0.0798 - accuracy: 0.9713 - precision: 0.9731 - recall: 0.9696 - val_loss: 0.2989 - val_accuracy: 0.9134 - val_precision: 0.9162 - val_recall: 0.9112 - lr: 1.2500e-04
Epoch 26/110
1194/1194 [==============================] - 488s 409ms/step - loss: 0.0778 - accuracy: 0.9718 - precision: 0.9736 - recall: 0.9702 - val_loss: 0.1496 - val_accuracy: 0.9560 - val_precision: 0.9572 - val_recall: 0.9547 - lr: 1.2500e-04
Epoch 27/110
1194/1194 [==============================] - 467s 391ms/step - loss: 0.0760 - accuracy: 0.9727 - precision: 0.9744 - recall: 0.9709 - val_loss: 0.2577 - val_accuracy: 0.9307 - val_precision: 0.9321 - val_recall: 0.9297 - lr: 1.2500e-04
Epoch 28/110
1194/1194 [==============================] - 483s 404ms/step - loss: 0.0759 - accuracy: 0.9724 - precision: 0.9742 - recall: 0.9709 - val_loss: 0.1486 - val_accuracy: 0.9550 - val_precision: 0.9565 - val_recall: 0.9535 - lr: 1.2500e-04
Epoch 29/110
1194/1194 [==============================] - 545s 457ms/step - loss: 0.0735 - accuracy: 0.9734 - precision: 0.9748 - recall: 0.9718 - val_loss: 0.2724 - val_accuracy: 0.9234 - val_precision: 0.9258 - val_recall: 0.9212 - lr: 1.2500e-04
Epoch 30/110
1194/1194 [==============================] - 484s 406ms/step - loss: 0.0732 - accuracy: 0.9738 - precision: 0.9752 - recall: 0.9722 - val_loss: 0.5247 - val_accuracy: 0.8794 - val_precision: 0.8820 - val_recall: 0.8777 - lr: 1.2500e-04
Epoch 31/110
1194/1194 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9743 - precision: 0.9757 - recall: 0.9729
Epoch 31: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
1194/1194 [==============================] - 526s 441ms/step - loss: 0.0718 - accuracy: 0.9743 - precision: 0.9757 - recall: 0.9729 - val_loss: 0.1597 - val_accuracy: 0.9531 - val_precision: 0.9547 - val_recall: 0.9514 - lr: 1.2500e-04
Epoch 32/110
1194/1194 [==============================] - 484s 405ms/step - loss: 0.0605 - accuracy: 0.9777 - precision: 0.9789 - recall: 0.9767 - val_loss: 0.1942 - val_accuracy: 0.9467 - val_precision: 0.9478 - val_recall: 0.9457 - lr: 6.2500e-05
Epoch 33/110
1194/1194 [==============================] - 535s 448ms/step - loss: 0.0583 - accuracy: 0.9790 - precision: 0.9801 - recall: 0.9780 - val_loss: 0.4421 - val_accuracy: 0.9043 - val_precision: 0.9059 - val_recall: 0.9034 - lr: 6.2500e-05
Epoch 34/110
1194/1194 [==============================] - 474s 397ms/step - loss: 0.0572 - accuracy: 0.9792 - precision: 0.9802 - recall: 0.9782 - val_loss: 0.6205 - val_accuracy: 0.8783 - val_precision: 0.8799 - val_recall: 0.8772 - lr: 6.2500e-05
Epoch 35/110
1194/1194 [==============================] - 536s 449ms/step - loss: 0.0569 - accuracy: 0.9796 - precision: 0.9806 - recall: 0.9784 - val_loss: 0.1776 - val_accuracy: 0.9531 - val_precision: 0.9540 - val_recall: 0.9524 - lr: 6.2500e-05
Epoch 36/110
1194/1194 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9797 - precision: 0.9809 - recall: 0.9787
Epoch 36: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
1194/1194 [==============================] - 540s 453ms/step - loss: 0.0568 - accuracy: 0.9797 - precision: 0.9809 - recall: 0.9787 - val_loss: 0.2470 - val_accuracy: 0.9377 - val_precision: 0.9392 - val_recall: 0.9368 - lr: 6.2500e-05
152766/152766 [==============================] - 598s 4ms/step - loss: 0.1496 - accuracy: 0.9560 - precision: 0.9572 - recall: 0.9547
WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 152766 batches). You may need to use the repeat() function when building your dataset.
Validation Loss: 0.1496, Validation Accuracy: 0.9560, Validation Precision: 0.9572, Validation Recall: 0.9547
2025-09-20 15:03:11.996123: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,32]
	 [[{{node inputs}}]]
2025-09-20 15:03:12.086831: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,64]
	 [[{{node inputs}}]]
2025-09-20 15:03:12.177769: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,128]
	 [[{{node inputs}}]]
2025-09-20 15:03:12.228918: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,32]
	 [[{{node inputs}}]]
2025-09-20 15:03:13.785338: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,32]
	 [[{{node inputs}}]]
2025-09-20 15:03:14.143820: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,64]
	 [[{{node inputs}}]]
2025-09-20 15:03:14.497691: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,128]
	 [[{{node inputs}}]]
2025-09-20 15:03:14.668469: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,32]
	 [[{{node inputs}}]]
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 20). These functions will not be directly callable after loading.
2025-09-20 15:03:22.462847: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.
2025-09-20 15:03:22.462876: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.
2025-09-20 15:03:22.464047: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp1x40e0an
2025-09-20 15:03:22.478674: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }
2025-09-20 15:03:22.478691: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp1x40e0an
2025-09-20 15:03:22.489631: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2025-09-20 15:03:22.538805: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2025-09-20 15:03:22.550747: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.
2025-09-20 15:03:22.921931: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmp1x40e0an
2025-09-20 15:03:23.022357: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 558312 microseconds.
2025-09-20 15:03:23.312592: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Input details: [{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 32, 44,  1], dtype=int32), 'shape_signature': array([-1, 32, 44,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
Output details: [{'name': 'StatefulPartitionedCall:0', 'index': 159, 'shape': array([1, 4], dtype=int32), 'shape_signature': array([-1,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
Training completed in time:  5:12:05.201567
4774/4774 [==============================] - 145s 30ms/step - loss: 0.0641 - accuracy: 0.9755 - precision: 0.9766 - recall: 0.9746
Training Accuracy:  0.9754984974861145
2047/2047 [==============================] - 62s 30ms/step - loss: 0.1496 - accuracy: 0.9560 - precision: 0.9572 - recall: 0.9547
Testing Accuracy:  0.955982506275177
2047/2047 [==============================] - 59s 28ms/step
0
0
Confusion matrix, without normalization
[[15506   159   459   268]
 [  238 15721    90   319]
 [  252   105 15878   120]
 [  213   492   167 15487]]
qt.xkb.compose: failed to create compose table

Classification report for MfCCs + CNN for fold1:
                               precision    recall  f1-score   support

                queen_absent       0.96      0.95      0.95     16392
queen_present_newly_accepted       0.95      0.96      0.96     16368
      queen_present_original       0.96      0.97      0.96     16355
      queen_present_rejected       0.96      0.95      0.95     16359

                    accuracy                           0.96     65474
                   macro avg       0.96      0.96      0.96     65474
                weighted avg       0.96      0.96      0.96     65474

rounded_predictions: [0 0 3 ... 3 3 3]
rounded_labels: [0 0 0 ... 3 3 3]
Confusion matrix, without normalization
[[15506   159   459   268]
 [  238 15721    90   319]
 [  252   105 15878   120]
 [  213   492   167 15487]]

Classification report:
                               precision    recall  f1-score   support

                queen_absent       0.96      0.95      0.95     16392
queen_present_newly_accepted       0.95      0.96      0.96     16368
      queen_present_original       0.96      0.97      0.96     16355
      queen_present_rejected       0.96      0.95      0.95     16359

                    accuracy                           0.96     65474
                   macro avg       0.96      0.96      0.96     65474
                weighted avg       0.96      0.96      0.96     65474

Accuracy:  0.9559825274154626
              precision    recall  f1-score       support
0              0.956629  0.945949  0.951259  16392.000000
1              0.954118  0.960472  0.957284  16368.000000
2              0.956852  0.970835  0.963793  16355.000000
3              0.956342  0.946696  0.951494  16359.000000
accuracy       0.955983  0.955983  0.955983      0.955983
macro avg      0.955985  0.955988  0.955958  65474.000000
weighted avg   0.955985  0.955983  0.955955  65474.000000

Process finished with exit code 0
